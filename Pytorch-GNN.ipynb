{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch\n",
    "## 1. Build the neural network\n",
    "### 1) Build A NN Model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "## Define Class\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 12),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Result:  tensor([[-0.0612,  0.0053, -0.0304,  0.0424, -0.0022, -0.0888,  0.0538,  0.1274,\n",
      "         -0.0222, -0.0036, -0.0424,  0.0291]], grad_fn=<AddmmBackward0>)\n",
      "pred_probab:  tensor([[0.0782, 0.0836, 0.0807, 0.0868, 0.0830, 0.0761, 0.0877, 0.0945, 0.0813,\n",
      "         0.0829, 0.0797, 0.0856]], grad_fn=<SoftmaxBackward0>)\n",
      "Predicited Class: tensor([7])\n"
     ]
    }
   ],
   "source": [
    "model = NN()\n",
    "\n",
    "print(model)\n",
    "\n",
    "X = torch.rand(1, 28, 28)\n",
    "logits = model(X)\n",
    "print(\"Result: \", logits)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(\"pred_probab: \", pred_probab)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicited Class: {y_pred}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2) Model Layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.2773, -0.0380,  0.1990,  0.0036,  0.0352, -0.6916, -0.2113,  0.0315,\n",
      "          0.1830, -0.5613, -0.0925,  0.2545,  0.2812,  0.1356,  0.3041,  0.1392,\n",
      "         -0.6648,  0.1825, -0.5288,  0.0067],\n",
      "        [ 0.4752, -0.0291,  0.3626,  0.1304,  0.0532, -0.4519, -0.2813,  0.0238,\n",
      "          0.2121, -0.5340, -0.1467,  0.4844,  0.0466,  0.1575,  0.5052,  0.3524,\n",
      "         -0.6622, -0.0446, -0.3488,  0.3965],\n",
      "        [ 0.1879,  0.2822,  0.3781,  0.1618, -0.1590, -0.3062, -0.3237,  0.1606,\n",
      "          0.1825, -0.7992, -0.1025,  0.2283, -0.1023,  0.0246,  0.1677,  0.1427,\n",
      "         -0.3895,  0.1979, -0.6698, -0.1093]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.2773, 0.0000, 0.1990, 0.0036, 0.0352, 0.0000, 0.0000, 0.0315, 0.1830,\n",
      "         0.0000, 0.0000, 0.2545, 0.2812, 0.1356, 0.3041, 0.1392, 0.0000, 0.1825,\n",
      "         0.0000, 0.0067],\n",
      "        [0.4752, 0.0000, 0.3626, 0.1304, 0.0532, 0.0000, 0.0000, 0.0238, 0.2121,\n",
      "         0.0000, 0.0000, 0.4844, 0.0466, 0.1575, 0.5052, 0.3524, 0.0000, 0.0000,\n",
      "         0.0000, 0.3965],\n",
      "        [0.1879, 0.2822, 0.3781, 0.1618, 0.0000, 0.0000, 0.0000, 0.1606, 0.1825,\n",
      "         0.0000, 0.0000, 0.2283, 0.0000, 0.0246, 0.1677, 0.1427, 0.0000, 0.1979,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.1401, -0.2220,  0.1288, -0.0182,  0.2030,  0.0044,  0.0046,  0.2070,\n          0.1709,  0.1050],\n        [-0.2172, -0.1837,  0.2571, -0.0160,  0.2552, -0.0059, -0.0054,  0.2456,\n          0.0862,  0.1759],\n        [-0.1179, -0.2739,  0.1828, -0.0356,  0.1859, -0.0231,  0.1972,  0.2984,\n          0.1610,  0.1459]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0824, 0.0759, 0.1078, 0.0931, 0.1161, 0.0952, 0.0952, 0.1166, 0.1124,\n         0.1053],\n        [0.0748, 0.0774, 0.1202, 0.0915, 0.1200, 0.0924, 0.0925, 0.1189, 0.1014,\n         0.1109],\n        [0.0816, 0.0698, 0.1102, 0.0886, 0.1105, 0.0897, 0.1118, 0.1237, 0.1078,\n         0.1062]], grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "pred_probab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | size: torch.Size([512, 784]) | Values: tensor([[-0.0157, -0.0124,  0.0347,  ...,  0.0302,  0.0143, -0.0017],\n",
      "        [ 0.0122,  0.0111,  0.0303,  ..., -0.0132,  0.0272,  0.0117]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | size: torch.Size([512]) | Values: tensor([0.0341, 0.0066], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | size: torch.Size([512, 512]) | Values: tensor([[ 0.0337, -0.0393, -0.0407,  ...,  0.0294,  0.0197, -0.0048],\n",
      "        [-0.0231,  0.0124,  0.0158,  ...,  0.0098, -0.0427, -0.0085]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | size: torch.Size([512]) | Values: tensor([0.0188, 0.0418], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | size: torch.Size([12, 512]) | Values: tensor([[ 0.0209, -0.0249, -0.0372,  ..., -0.0244,  0.0237,  0.0219],\n",
      "        [-0.0375,  0.0387, -0.0218,  ..., -0.0368, -0.0158, -0.0118]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | size: torch.Size([12]) | Values: tensor([-0.0329,  0.0058], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure:  {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | size: {param.size()} | Values: {param[:2]} \\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Automatic Differentiation with torch.autograde\n",
    "When training neural network, the most frequently used algorithm is back propagation. In this algorithm, parameters(model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.\n",
    "\n",
    "PyTorch has build the torch.autograd to compute those gradients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x0000011ED76117C0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x0000011ED7611820>\n"
     ]
    }
   ],
   "source": [
    "# to check the function for gradient\n",
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Computing Gradients\n",
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need $\\frac{\\partial loss}{\\partial w}$ and $\\frac{\\partial loss}{\\partial b}$ under some fixed values of x and y. We call loss.backward() to compute those derivatives, and then retrieve the values from w.grad and b.grad.\n",
    "\n",
    "* We can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.\n",
    "* We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "loss.backward()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1949, 0.3180, 0.2546],\n",
      "        [0.1949, 0.3180, 0.2546],\n",
      "        [0.1949, 0.3180, 0.2546],\n",
      "        [0.1949, 0.3180, 0.2546],\n",
      "        [0.1949, 0.3180, 0.2546]])\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1949, 0.3180, 0.2546])\n"
     ]
    }
   ],
   "source": [
    "print(b.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Disabling Gradient Tracking\n",
    "Be default, all tensors with requires_grad=True are tracking their computational history and support gradient computation. However, there are some cases when we do not need do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to *forward* computations through the network. We can stop tracking computations by surrounding our computation code with torch.no_grad() block:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another way to achieve the same result is to use the detach() on the tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tensor Gradients and Jacobian Products"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 0., 1., 0.]], requires_grad=True)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "inp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 1., 1., 1.],\n        [1., 4., 1., 1.],\n        [1., 1., 4., 1.],\n        [1., 1., 1., 4.],\n        [1., 1., 1., 1.]], grad_fn=<TBackward0>)"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = (inp + 1).pow(2).t()\n",
    "out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"Second call\\n{inp.grad}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"Call after zeroing gradients\\n{inp.grad}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Optimizing Model Parameters\n",
    "Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess *(loss)*, collects the derivatives of the error with respect to its parameters, and optimizes these parameters using gradient descent. For a more detailed walkthrough of this process, check out the video: https://www.youtube.com/watch?v=tIeHLnjs5U8.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prerequisite Code\n",
    "We load the code from the previous section on datasets & Data_loads and build model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NN()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Optimization Loop\n",
    "Each iteration of optimization loop is called an **epoch**.\n",
    "Each epoch consists of two main parts:\n",
    "* **The Train Loop** - iterate over the training dataset and try to converge to optimal parameters.\n",
    "* **The Validation/Test Loop** -iterate over the test dataset to check if model performance is improving.\n",
    "\n",
    "#### Loss Function\n",
    "* **nn.MSELoss** -Mean Square Error for regression task.\n",
    "* **nn.NLLLoss** -Negative Log Likelihood for classfication.\n",
    "* **nn.CrossEntropyLoss** -combines nn.LogSoftmax and nn.NLLLoss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "* Call **optimizer.zero_grad()** to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "* Backpropagate the prediction loss with a call to **loss.backward()**. PyToch deposits the gradients of the loss w.r.t. each parameter.\n",
    "* Once we have our gradients, we call **optimizer.step()** to adjust the parameters by the gradients collected in the backward pass.\n",
    "\n",
    "#### Full Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current: >5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n ACC: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------------------\n",
      "loss: 2.307213 [   64/60000]\n",
      "loss: 2.293616 [ 6464/60000]\n",
      "loss: 2.280282 [12864/60000]\n",
      "loss: 2.262713 [19264/60000]\n",
      "loss: 2.244120 [25664/60000]\n",
      "loss: 2.214276 [32064/60000]\n",
      "loss: 2.224720 [38464/60000]\n",
      "loss: 2.197532 [44864/60000]\n",
      "loss: 2.186252 [51264/60000]\n",
      "loss: 2.145954 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 34.4%, Avg loss: 2.151113\n",
      "\n",
      "Epoch 2\n",
      "---------------------------------\n",
      "loss: 2.165578 [   64/60000]\n",
      "loss: 2.152183 [ 6464/60000]\n",
      "loss: 2.101547 [12864/60000]\n",
      "loss: 2.103894 [19264/60000]\n",
      "loss: 2.049453 [25664/60000]\n",
      "loss: 1.991416 [32064/60000]\n",
      "loss: 2.016410 [38464/60000]\n",
      "loss: 1.944982 [44864/60000]\n",
      "loss: 1.941644 [51264/60000]\n",
      "loss: 1.861235 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 56.1%, Avg loss: 1.867334\n",
      "\n",
      "Epoch 3\n",
      "---------------------------------\n",
      "loss: 1.905179 [   64/60000]\n",
      "loss: 1.872437 [ 6464/60000]\n",
      "loss: 1.764631 [12864/60000]\n",
      "loss: 1.790527 [19264/60000]\n",
      "loss: 1.675589 [25664/60000]\n",
      "loss: 1.639269 [32064/60000]\n",
      "loss: 1.654220 [38464/60000]\n",
      "loss: 1.568351 [44864/60000]\n",
      "loss: 1.585990 [51264/60000]\n",
      "loss: 1.482064 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 59.9%, Avg loss: 1.501469\n",
      "\n",
      "Epoch 4\n",
      "---------------------------------\n",
      "loss: 1.572074 [   64/60000]\n",
      "loss: 1.537944 [ 6464/60000]\n",
      "loss: 1.398099 [12864/60000]\n",
      "loss: 1.456771 [19264/60000]\n",
      "loss: 1.339452 [25664/60000]\n",
      "loss: 1.341653 [32064/60000]\n",
      "loss: 1.356410 [38464/60000]\n",
      "loss: 1.287481 [44864/60000]\n",
      "loss: 1.311076 [51264/60000]\n",
      "loss: 1.223823 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 62.9%, Avg loss: 1.244704\n",
      "\n",
      "Epoch 5\n",
      "---------------------------------\n",
      "loss: 1.322077 [   64/60000]\n",
      "loss: 1.305748 [ 6464/60000]\n",
      "loss: 1.147096 [12864/60000]\n",
      "loss: 1.243634 [19264/60000]\n",
      "loss: 1.124963 [25664/60000]\n",
      "loss: 1.144599 [32064/60000]\n",
      "loss: 1.174196 [38464/60000]\n",
      "loss: 1.111847 [44864/60000]\n",
      "loss: 1.137490 [51264/60000]\n",
      "loss: 1.069722 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 64.6%, Avg loss: 1.084113\n",
      "\n",
      "Epoch 6\n",
      "---------------------------------\n",
      "loss: 1.153589 [   64/60000]\n",
      "loss: 1.157742 [ 6464/60000]\n",
      "loss: 0.981205 [12864/60000]\n",
      "loss: 1.108710 [19264/60000]\n",
      "loss: 0.991076 [25664/60000]\n",
      "loss: 1.010383 [32064/60000]\n",
      "loss: 1.058137 [38464/60000]\n",
      "loss: 0.998973 [44864/60000]\n",
      "loss: 1.023118 [51264/60000]\n",
      "loss: 0.971629 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 65.9%, Avg loss: 0.979328\n",
      "\n",
      "Epoch 7\n",
      "---------------------------------\n",
      "loss: 1.034896 [   64/60000]\n",
      "loss: 1.060930 [ 6464/60000]\n",
      "loss: 0.867359 [12864/60000]\n",
      "loss: 1.017953 [19264/60000]\n",
      "loss: 0.905291 [25664/60000]\n",
      "loss: 0.915065 [32064/60000]\n",
      "loss: 0.980179 [38464/60000]\n",
      "loss: 0.925166 [44864/60000]\n",
      "loss: 0.944491 [51264/60000]\n",
      "loss: 0.905309 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 67.3%, Avg loss: 0.907821\n",
      "\n",
      "Epoch 8\n",
      "---------------------------------\n",
      "loss: 0.947603 [   64/60000]\n",
      "loss: 0.994508 [ 6464/60000]\n",
      "loss: 0.786055 [12864/60000]\n",
      "loss: 0.953926 [19264/60000]\n",
      "loss: 0.847435 [25664/60000]\n",
      "loss: 0.845453 [32064/60000]\n",
      "loss: 0.924530 [38464/60000]\n",
      "loss: 0.875658 [44864/60000]\n",
      "loss: 0.888362 [51264/60000]\n",
      "loss: 0.857655 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 68.5%, Avg loss: 0.856419\n",
      "\n",
      "Epoch 9\n",
      "---------------------------------\n",
      "loss: 0.880898 [   64/60000]\n",
      "loss: 0.945367 [ 6464/60000]\n",
      "loss: 0.725388 [12864/60000]\n",
      "loss: 0.906321 [19264/60000]\n",
      "loss: 0.805604 [25664/60000]\n",
      "loss: 0.793110 [32064/60000]\n",
      "loss: 0.882321 [38464/60000]\n",
      "loss: 0.840705 [44864/60000]\n",
      "loss: 0.846647 [51264/60000]\n",
      "loss: 0.821249 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 69.8%, Avg loss: 0.817332\n",
      "\n",
      "Epoch 10\n",
      "---------------------------------\n",
      "loss: 0.827596 [   64/60000]\n",
      "loss: 0.906222 [ 6464/60000]\n",
      "loss: 0.677956 [12864/60000]\n",
      "loss: 0.869343 [19264/60000]\n",
      "loss: 0.773326 [25664/60000]\n",
      "loss: 0.752605 [32064/60000]\n",
      "loss: 0.848086 [38464/60000]\n",
      "loss: 0.814198 [44864/60000]\n",
      "loss: 0.814023 [51264/60000]\n",
      "loss: 0.792022 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 71.0%, Avg loss: 0.786037\n",
      "\n",
      "Epoch 11\n",
      "---------------------------------\n",
      "loss: 0.783127 [   64/60000]\n",
      "loss: 0.872990 [ 6464/60000]\n",
      "loss: 0.639473 [12864/60000]\n",
      "loss: 0.839671 [19264/60000]\n",
      "loss: 0.746943 [25664/60000]\n",
      "loss: 0.720288 [32064/60000]\n",
      "loss: 0.818954 [38464/60000]\n",
      "loss: 0.792732 [44864/60000]\n",
      "loss: 0.787524 [51264/60000]\n",
      "loss: 0.767445 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 72.5%, Avg loss: 0.759825\n",
      "\n",
      "Epoch 12\n",
      "---------------------------------\n",
      "loss: 0.744890 [   64/60000]\n",
      "loss: 0.843505 [ 6464/60000]\n",
      "loss: 0.607372 [12864/60000]\n",
      "loss: 0.815189 [19264/60000]\n",
      "loss: 0.724631 [25664/60000]\n",
      "loss: 0.693766 [32064/60000]\n",
      "loss: 0.793228 [38464/60000]\n",
      "loss: 0.774386 [44864/60000]\n",
      "loss: 0.765177 [51264/60000]\n",
      "loss: 0.746062 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 73.6%, Avg loss: 0.737073\n",
      "\n",
      "Epoch 13\n",
      "---------------------------------\n",
      "loss: 0.711248 [   64/60000]\n",
      "loss: 0.816668 [ 6464/60000]\n",
      "loss: 0.579868 [12864/60000]\n",
      "loss: 0.794311 [19264/60000]\n",
      "loss: 0.705170 [25664/60000]\n",
      "loss: 0.671699 [32064/60000]\n",
      "loss: 0.769923 [38464/60000]\n",
      "loss: 0.757947 [44864/60000]\n",
      "loss: 0.745910 [51264/60000]\n",
      "loss: 0.727010 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 74.5%, Avg loss: 0.716826\n",
      "\n",
      "Epoch 14\n",
      "---------------------------------\n",
      "loss: 0.681370 [   64/60000]\n",
      "loss: 0.791969 [ 6464/60000]\n",
      "loss: 0.555943 [12864/60000]\n",
      "loss: 0.776006 [19264/60000]\n",
      "loss: 0.688019 [25664/60000]\n",
      "loss: 0.653120 [32064/60000]\n",
      "loss: 0.748289 [38464/60000]\n",
      "loss: 0.743147 [44864/60000]\n",
      "loss: 0.728978 [51264/60000]\n",
      "loss: 0.709664 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 75.2%, Avg loss: 0.698512\n",
      "\n",
      "Epoch 15\n",
      "---------------------------------\n",
      "loss: 0.654608 [   64/60000]\n",
      "loss: 0.769000 [ 6464/60000]\n",
      "loss: 0.534889 [12864/60000]\n",
      "loss: 0.759704 [19264/60000]\n",
      "loss: 0.672809 [25664/60000]\n",
      "loss: 0.637293 [32064/60000]\n",
      "loss: 0.727975 [38464/60000]\n",
      "loss: 0.729735 [44864/60000]\n",
      "loss: 0.714145 [51264/60000]\n",
      "loss: 0.693628 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 75.9%, Avg loss: 0.681810\n",
      "\n",
      "Epoch 16\n",
      "---------------------------------\n",
      "loss: 0.630448 [   64/60000]\n",
      "loss: 0.747672 [ 6464/60000]\n",
      "loss: 0.516388 [12864/60000]\n",
      "loss: 0.745009 [19264/60000]\n",
      "loss: 0.659342 [25664/60000]\n",
      "loss: 0.623515 [32064/60000]\n",
      "loss: 0.709078 [38464/60000]\n",
      "loss: 0.717649 [44864/60000]\n",
      "loss: 0.700968 [51264/60000]\n",
      "loss: 0.678679 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 76.5%, Avg loss: 0.666521\n",
      "\n",
      "Epoch 17\n",
      "---------------------------------\n",
      "loss: 0.608738 [   64/60000]\n",
      "loss: 0.727912 [ 6464/60000]\n",
      "loss: 0.499973 [12864/60000]\n",
      "loss: 0.731572 [19264/60000]\n",
      "loss: 0.647345 [25664/60000]\n",
      "loss: 0.611546 [32064/60000]\n",
      "loss: 0.691495 [38464/60000]\n",
      "loss: 0.706813 [44864/60000]\n",
      "loss: 0.689488 [51264/60000]\n",
      "loss: 0.664636 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 77.2%, Avg loss: 0.652498\n",
      "\n",
      "Epoch 18\n",
      "---------------------------------\n",
      "loss: 0.589252 [   64/60000]\n",
      "loss: 0.709583 [ 6464/60000]\n",
      "loss: 0.485257 [12864/60000]\n",
      "loss: 0.719183 [19264/60000]\n",
      "loss: 0.636725 [25664/60000]\n",
      "loss: 0.601142 [32064/60000]\n",
      "loss: 0.675085 [38464/60000]\n",
      "loss: 0.697129 [44864/60000]\n",
      "loss: 0.679630 [51264/60000]\n",
      "loss: 0.651418 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 77.6%, Avg loss: 0.639611\n",
      "\n",
      "Epoch 19\n",
      "---------------------------------\n",
      "loss: 0.571659 [   64/60000]\n",
      "loss: 0.692599 [ 6464/60000]\n",
      "loss: 0.472038 [12864/60000]\n",
      "loss: 0.707755 [19264/60000]\n",
      "loss: 0.627409 [25664/60000]\n",
      "loss: 0.591918 [32064/60000]\n",
      "loss: 0.659803 [38464/60000]\n",
      "loss: 0.688715 [44864/60000]\n",
      "loss: 0.671164 [51264/60000]\n",
      "loss: 0.638952 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 78.1%, Avg loss: 0.627780\n",
      "\n",
      "Epoch 20\n",
      "---------------------------------\n",
      "loss: 0.555688 [   64/60000]\n",
      "loss: 0.676908 [ 6464/60000]\n",
      "loss: 0.460101 [12864/60000]\n",
      "loss: 0.697106 [19264/60000]\n",
      "loss: 0.618973 [25664/60000]\n",
      "loss: 0.583672 [32064/60000]\n",
      "loss: 0.645622 [38464/60000]\n",
      "loss: 0.681652 [44864/60000]\n",
      "loss: 0.663930 [51264/60000]\n",
      "loss: 0.627329 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 78.5%, Avg loss: 0.616933\n",
      "\n",
      "Epoch 21\n",
      "---------------------------------\n",
      "loss: 0.541177 [   64/60000]\n",
      "loss: 0.662507 [ 6464/60000]\n",
      "loss: 0.449169 [12864/60000]\n",
      "loss: 0.687318 [19264/60000]\n",
      "loss: 0.611210 [25664/60000]\n",
      "loss: 0.576247 [32064/60000]\n",
      "loss: 0.632538 [38464/60000]\n",
      "loss: 0.675640 [44864/60000]\n",
      "loss: 0.657700 [51264/60000]\n",
      "loss: 0.616331 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 78.9%, Avg loss: 0.606968\n",
      "\n",
      "Epoch 22\n",
      "---------------------------------\n",
      "loss: 0.527939 [   64/60000]\n",
      "loss: 0.649217 [ 6464/60000]\n",
      "loss: 0.439178 [12864/60000]\n",
      "loss: 0.678155 [19264/60000]\n",
      "loss: 0.604016 [25664/60000]\n",
      "loss: 0.569564 [32064/60000]\n",
      "loss: 0.620495 [38464/60000]\n",
      "loss: 0.670691 [44864/60000]\n",
      "loss: 0.652371 [51264/60000]\n",
      "loss: 0.605809 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 79.3%, Avg loss: 0.597802\n",
      "\n",
      "Epoch 23\n",
      "---------------------------------\n",
      "loss: 0.515829 [   64/60000]\n",
      "loss: 0.636962 [ 6464/60000]\n",
      "loss: 0.430046 [12864/60000]\n",
      "loss: 0.669524 [19264/60000]\n",
      "loss: 0.597146 [25664/60000]\n",
      "loss: 0.563348 [32064/60000]\n",
      "loss: 0.609460 [38464/60000]\n",
      "loss: 0.666711 [44864/60000]\n",
      "loss: 0.647745 [51264/60000]\n",
      "loss: 0.595734 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 79.5%, Avg loss: 0.589355\n",
      "\n",
      "Epoch 24\n",
      "---------------------------------\n",
      "loss: 0.504647 [   64/60000]\n",
      "loss: 0.625617 [ 6464/60000]\n",
      "loss: 0.421632 [12864/60000]\n",
      "loss: 0.661427 [19264/60000]\n",
      "loss: 0.590558 [25664/60000]\n",
      "loss: 0.557492 [32064/60000]\n",
      "loss: 0.599209 [38464/60000]\n",
      "loss: 0.663475 [44864/60000]\n",
      "loss: 0.643849 [51264/60000]\n",
      "loss: 0.586075 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 79.7%, Avg loss: 0.581558\n",
      "\n",
      "Epoch 25\n",
      "---------------------------------\n",
      "loss: 0.494259 [   64/60000]\n",
      "loss: 0.615123 [ 6464/60000]\n",
      "loss: 0.413891 [12864/60000]\n",
      "loss: 0.653777 [19264/60000]\n",
      "loss: 0.584158 [25664/60000]\n",
      "loss: 0.551955 [32064/60000]\n",
      "loss: 0.589611 [38464/60000]\n",
      "loss: 0.660969 [44864/60000]\n",
      "loss: 0.640545 [51264/60000]\n",
      "loss: 0.576815 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 80.0%, Avg loss: 0.574349\n",
      "\n",
      "Epoch 26\n",
      "---------------------------------\n",
      "loss: 0.484524 [   64/60000]\n",
      "loss: 0.605416 [ 6464/60000]\n",
      "loss: 0.406766 [12864/60000]\n",
      "loss: 0.646490 [19264/60000]\n",
      "loss: 0.577938 [25664/60000]\n",
      "loss: 0.546656 [32064/60000]\n",
      "loss: 0.580674 [38464/60000]\n",
      "loss: 0.659249 [44864/60000]\n",
      "loss: 0.637744 [51264/60000]\n",
      "loss: 0.567889 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 80.2%, Avg loss: 0.567673\n",
      "\n",
      "Epoch 27\n",
      "---------------------------------\n",
      "loss: 0.475401 [   64/60000]\n",
      "loss: 0.596413 [ 6464/60000]\n",
      "loss: 0.400154 [12864/60000]\n",
      "loss: 0.639524 [19264/60000]\n",
      "loss: 0.571865 [25664/60000]\n",
      "loss: 0.541508 [32064/60000]\n",
      "loss: 0.572421 [38464/60000]\n",
      "loss: 0.658117 [44864/60000]\n",
      "loss: 0.635318 [51264/60000]\n",
      "loss: 0.559289 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 80.5%, Avg loss: 0.561477\n",
      "\n",
      "Epoch 28\n",
      "---------------------------------\n",
      "loss: 0.466835 [   64/60000]\n",
      "loss: 0.588080 [ 6464/60000]\n",
      "loss: 0.393984 [12864/60000]\n",
      "loss: 0.632964 [19264/60000]\n",
      "loss: 0.565956 [25664/60000]\n",
      "loss: 0.536462 [32064/60000]\n",
      "loss: 0.564803 [38464/60000]\n",
      "loss: 0.657442 [44864/60000]\n",
      "loss: 0.633194 [51264/60000]\n",
      "loss: 0.550990 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 80.7%, Avg loss: 0.555713\n",
      "\n",
      "Epoch 29\n",
      "---------------------------------\n",
      "loss: 0.458783 [   64/60000]\n",
      "loss: 0.580318 [ 6464/60000]\n",
      "loss: 0.388240 [12864/60000]\n",
      "loss: 0.626714 [19264/60000]\n",
      "loss: 0.560174 [25664/60000]\n",
      "loss: 0.531515 [32064/60000]\n",
      "loss: 0.557716 [38464/60000]\n",
      "loss: 0.657215 [44864/60000]\n",
      "loss: 0.631281 [51264/60000]\n",
      "loss: 0.542932 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 80.9%, Avg loss: 0.550348\n",
      "\n",
      "Epoch 30\n",
      "---------------------------------\n",
      "loss: 0.451175 [   64/60000]\n",
      "loss: 0.573120 [ 6464/60000]\n",
      "loss: 0.382825 [12864/60000]\n",
      "loss: 0.620732 [19264/60000]\n",
      "loss: 0.554477 [25664/60000]\n",
      "loss: 0.526582 [32064/60000]\n",
      "loss: 0.551134 [38464/60000]\n",
      "loss: 0.657324 [44864/60000]\n",
      "loss: 0.629521 [51264/60000]\n",
      "loss: 0.535071 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 81.1%, Avg loss: 0.545340\n",
      "\n",
      "Epoch 31\n",
      "---------------------------------\n",
      "loss: 0.443937 [   64/60000]\n",
      "loss: 0.566446 [ 6464/60000]\n",
      "loss: 0.377726 [12864/60000]\n",
      "loss: 0.615047 [19264/60000]\n",
      "loss: 0.548831 [25664/60000]\n",
      "loss: 0.521684 [32064/60000]\n",
      "loss: 0.545014 [38464/60000]\n",
      "loss: 0.657622 [44864/60000]\n",
      "loss: 0.627857 [51264/60000]\n",
      "loss: 0.527483 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 81.2%, Avg loss: 0.540658\n",
      "\n",
      "Epoch 32\n",
      "---------------------------------\n",
      "loss: 0.437000 [   64/60000]\n",
      "loss: 0.560239 [ 6464/60000]\n",
      "loss: 0.372937 [12864/60000]\n",
      "loss: 0.609630 [19264/60000]\n",
      "loss: 0.543464 [25664/60000]\n",
      "loss: 0.516750 [32064/60000]\n",
      "loss: 0.539347 [38464/60000]\n",
      "loss: 0.658041 [44864/60000]\n",
      "loss: 0.626266 [51264/60000]\n",
      "loss: 0.520159 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 81.2%, Avg loss: 0.536279\n",
      "\n",
      "Epoch 33\n",
      "---------------------------------\n",
      "loss: 0.430333 [   64/60000]\n",
      "loss: 0.554483 [ 6464/60000]\n",
      "loss: 0.368437 [12864/60000]\n",
      "loss: 0.604394 [19264/60000]\n",
      "loss: 0.538152 [25664/60000]\n",
      "loss: 0.511943 [32064/60000]\n",
      "loss: 0.534071 [38464/60000]\n",
      "loss: 0.658447 [44864/60000]\n",
      "loss: 0.624720 [51264/60000]\n",
      "loss: 0.513153 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 81.4%, Avg loss: 0.532179\n",
      "\n",
      "Epoch 34\n",
      "---------------------------------\n",
      "loss: 0.423982 [   64/60000]\n",
      "loss: 0.549092 [ 6464/60000]\n",
      "loss: 0.364142 [12864/60000]\n",
      "loss: 0.599329 [19264/60000]\n",
      "loss: 0.532962 [25664/60000]\n",
      "loss: 0.507220 [32064/60000]\n",
      "loss: 0.529063 [38464/60000]\n",
      "loss: 0.658895 [44864/60000]\n",
      "loss: 0.623250 [51264/60000]\n",
      "loss: 0.506370 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 81.4%, Avg loss: 0.528335\n",
      "\n",
      "Epoch 35\n",
      "---------------------------------\n",
      "loss: 0.417913 [   64/60000]\n",
      "loss: 0.544074 [ 6464/60000]\n",
      "loss: 0.360057 [12864/60000]\n",
      "loss: 0.594458 [19264/60000]\n",
      "loss: 0.527931 [25664/60000]\n",
      "loss: 0.502583 [32064/60000]\n",
      "loss: 0.524344 [38464/60000]\n",
      "loss: 0.659364 [44864/60000]\n",
      "loss: 0.621791 [51264/60000]\n",
      "loss: 0.499939 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 81.5%, Avg loss: 0.524720\n",
      "\n",
      "Epoch 36\n",
      "---------------------------------\n",
      "loss: 0.412088 [   64/60000]\n",
      "loss: 0.539380 [ 6464/60000]\n",
      "loss: 0.356164 [12864/60000]\n",
      "loss: 0.589743 [19264/60000]\n",
      "loss: 0.523001 [25664/60000]\n",
      "loss: 0.498001 [32064/60000]\n",
      "loss: 0.519901 [38464/60000]\n",
      "loss: 0.659788 [44864/60000]\n",
      "loss: 0.620338 [51264/60000]\n",
      "loss: 0.493787 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 81.6%, Avg loss: 0.521317\n",
      "\n",
      "Epoch 37\n",
      "---------------------------------\n",
      "loss: 0.406471 [   64/60000]\n",
      "loss: 0.534986 [ 6464/60000]\n",
      "loss: 0.352469 [12864/60000]\n",
      "loss: 0.585183 [19264/60000]\n",
      "loss: 0.518229 [25664/60000]\n",
      "loss: 0.493486 [32064/60000]\n",
      "loss: 0.515711 [38464/60000]\n",
      "loss: 0.660158 [44864/60000]\n",
      "loss: 0.618825 [51264/60000]\n",
      "loss: 0.487944 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 81.6%, Avg loss: 0.518106\n",
      "\n",
      "Epoch 38\n",
      "---------------------------------\n",
      "loss: 0.401072 [   64/60000]\n",
      "loss: 0.530883 [ 6464/60000]\n",
      "loss: 0.348957 [12864/60000]\n",
      "loss: 0.580790 [19264/60000]\n",
      "loss: 0.513608 [25664/60000]\n",
      "loss: 0.489095 [32064/60000]\n",
      "loss: 0.511739 [38464/60000]\n",
      "loss: 0.660411 [44864/60000]\n",
      "loss: 0.617325 [51264/60000]\n",
      "loss: 0.482340 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 81.8%, Avg loss: 0.515069\n",
      "\n",
      "Epoch 39\n",
      "---------------------------------\n",
      "loss: 0.395882 [   64/60000]\n",
      "loss: 0.527020 [ 6464/60000]\n",
      "loss: 0.345618 [12864/60000]\n",
      "loss: 0.576550 [19264/60000]\n",
      "loss: 0.509104 [25664/60000]\n",
      "loss: 0.484794 [32064/60000]\n",
      "loss: 0.507959 [38464/60000]\n",
      "loss: 0.660563 [44864/60000]\n",
      "loss: 0.615843 [51264/60000]\n",
      "loss: 0.477028 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 81.8%, Avg loss: 0.512190\n",
      "\n",
      "Epoch 40\n",
      "---------------------------------\n",
      "loss: 0.390874 [   64/60000]\n",
      "loss: 0.523412 [ 6464/60000]\n",
      "loss: 0.342449 [12864/60000]\n",
      "loss: 0.572436 [19264/60000]\n",
      "loss: 0.504767 [25664/60000]\n",
      "loss: 0.480613 [32064/60000]\n",
      "loss: 0.504356 [38464/60000]\n",
      "loss: 0.660584 [44864/60000]\n",
      "loss: 0.614312 [51264/60000]\n",
      "loss: 0.471970 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 81.9%, Avg loss: 0.509454\n",
      "\n",
      "Epoch 41\n",
      "---------------------------------\n",
      "loss: 0.386020 [   64/60000]\n",
      "loss: 0.519994 [ 6464/60000]\n",
      "loss: 0.339401 [12864/60000]\n",
      "loss: 0.568479 [19264/60000]\n",
      "loss: 0.500556 [25664/60000]\n",
      "loss: 0.476595 [32064/60000]\n",
      "loss: 0.500932 [38464/60000]\n",
      "loss: 0.660468 [44864/60000]\n",
      "loss: 0.612754 [51264/60000]\n",
      "loss: 0.467184 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.0%, Avg loss: 0.506848\n",
      "\n",
      "Epoch 42\n",
      "---------------------------------\n",
      "loss: 0.381317 [   64/60000]\n",
      "loss: 0.516799 [ 6464/60000]\n",
      "loss: 0.336513 [12864/60000]\n",
      "loss: 0.564626 [19264/60000]\n",
      "loss: 0.496438 [25664/60000]\n",
      "loss: 0.472732 [32064/60000]\n",
      "loss: 0.497652 [38464/60000]\n",
      "loss: 0.660186 [44864/60000]\n",
      "loss: 0.611160 [51264/60000]\n",
      "loss: 0.462622 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.0%, Avg loss: 0.504362\n",
      "\n",
      "Epoch 43\n",
      "---------------------------------\n",
      "loss: 0.376772 [   64/60000]\n",
      "loss: 0.513792 [ 6464/60000]\n",
      "loss: 0.333733 [12864/60000]\n",
      "loss: 0.560912 [19264/60000]\n",
      "loss: 0.492476 [25664/60000]\n",
      "loss: 0.468991 [32064/60000]\n",
      "loss: 0.494518 [38464/60000]\n",
      "loss: 0.659727 [44864/60000]\n",
      "loss: 0.609527 [51264/60000]\n",
      "loss: 0.458325 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.2%, Avg loss: 0.501994\n",
      "\n",
      "Epoch 44\n",
      "---------------------------------\n",
      "loss: 0.372399 [   64/60000]\n",
      "loss: 0.510923 [ 6464/60000]\n",
      "loss: 0.331072 [12864/60000]\n",
      "loss: 0.557351 [19264/60000]\n",
      "loss: 0.488666 [25664/60000]\n",
      "loss: 0.465383 [32064/60000]\n",
      "loss: 0.491504 [38464/60000]\n",
      "loss: 0.659098 [44864/60000]\n",
      "loss: 0.607939 [51264/60000]\n",
      "loss: 0.454228 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.2%, Avg loss: 0.499731\n",
      "\n",
      "Epoch 45\n",
      "---------------------------------\n",
      "loss: 0.368164 [   64/60000]\n",
      "loss: 0.508201 [ 6464/60000]\n",
      "loss: 0.328495 [12864/60000]\n",
      "loss: 0.553913 [19264/60000]\n",
      "loss: 0.484955 [25664/60000]\n",
      "loss: 0.461947 [32064/60000]\n",
      "loss: 0.488626 [38464/60000]\n",
      "loss: 0.658317 [44864/60000]\n",
      "loss: 0.606356 [51264/60000]\n",
      "loss: 0.450358 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.3%, Avg loss: 0.497565\n",
      "\n",
      "Epoch 46\n",
      "---------------------------------\n",
      "loss: 0.364030 [   64/60000]\n",
      "loss: 0.505577 [ 6464/60000]\n",
      "loss: 0.326027 [12864/60000]\n",
      "loss: 0.550604 [19264/60000]\n",
      "loss: 0.481371 [25664/60000]\n",
      "loss: 0.458640 [32064/60000]\n",
      "loss: 0.485891 [38464/60000]\n",
      "loss: 0.657431 [44864/60000]\n",
      "loss: 0.604756 [51264/60000]\n",
      "loss: 0.446728 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.4%, Avg loss: 0.495487\n",
      "\n",
      "Epoch 47\n",
      "---------------------------------\n",
      "loss: 0.360026 [   64/60000]\n",
      "loss: 0.503102 [ 6464/60000]\n",
      "loss: 0.323645 [12864/60000]\n",
      "loss: 0.547398 [19264/60000]\n",
      "loss: 0.477944 [25664/60000]\n",
      "loss: 0.455466 [32064/60000]\n",
      "loss: 0.483258 [38464/60000]\n",
      "loss: 0.656408 [44864/60000]\n",
      "loss: 0.603184 [51264/60000]\n",
      "loss: 0.443298 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.5%, Avg loss: 0.493494\n",
      "\n",
      "Epoch 48\n",
      "---------------------------------\n",
      "loss: 0.356188 [   64/60000]\n",
      "loss: 0.500747 [ 6464/60000]\n",
      "loss: 0.321351 [12864/60000]\n",
      "loss: 0.544257 [19264/60000]\n",
      "loss: 0.474695 [25664/60000]\n",
      "loss: 0.452436 [32064/60000]\n",
      "loss: 0.480710 [38464/60000]\n",
      "loss: 0.655253 [44864/60000]\n",
      "loss: 0.601598 [51264/60000]\n",
      "loss: 0.440091 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.5%, Avg loss: 0.491577\n",
      "\n",
      "Epoch 49\n",
      "---------------------------------\n",
      "loss: 0.352468 [   64/60000]\n",
      "loss: 0.498513 [ 6464/60000]\n",
      "loss: 0.319118 [12864/60000]\n",
      "loss: 0.541251 [19264/60000]\n",
      "loss: 0.471598 [25664/60000]\n",
      "loss: 0.449523 [32064/60000]\n",
      "loss: 0.478215 [38464/60000]\n",
      "loss: 0.653988 [44864/60000]\n",
      "loss: 0.600007 [51264/60000]\n",
      "loss: 0.437050 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.6%, Avg loss: 0.489730\n",
      "\n",
      "Epoch 50\n",
      "---------------------------------\n",
      "loss: 0.348847 [   64/60000]\n",
      "loss: 0.496368 [ 6464/60000]\n",
      "loss: 0.317035 [12864/60000]\n",
      "loss: 0.538345 [19264/60000]\n",
      "loss: 0.468629 [25664/60000]\n",
      "loss: 0.446748 [32064/60000]\n",
      "loss: 0.475838 [38464/60000]\n",
      "loss: 0.652652 [44864/60000]\n",
      "loss: 0.598433 [51264/60000]\n",
      "loss: 0.434197 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.7%, Avg loss: 0.487940\n",
      "\n",
      "Epoch 51\n",
      "---------------------------------\n",
      "loss: 0.345386 [   64/60000]\n",
      "loss: 0.494269 [ 6464/60000]\n",
      "loss: 0.315090 [12864/60000]\n",
      "loss: 0.535482 [19264/60000]\n",
      "loss: 0.465786 [25664/60000]\n",
      "loss: 0.444024 [32064/60000]\n",
      "loss: 0.473477 [38464/60000]\n",
      "loss: 0.651304 [44864/60000]\n",
      "loss: 0.596882 [51264/60000]\n",
      "loss: 0.431504 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.7%, Avg loss: 0.486216\n",
      "\n",
      "Epoch 52\n",
      "---------------------------------\n",
      "loss: 0.342010 [   64/60000]\n",
      "loss: 0.492220 [ 6464/60000]\n",
      "loss: 0.313212 [12864/60000]\n",
      "loss: 0.532740 [19264/60000]\n",
      "loss: 0.463025 [25664/60000]\n",
      "loss: 0.441479 [32064/60000]\n",
      "loss: 0.471227 [38464/60000]\n",
      "loss: 0.649907 [44864/60000]\n",
      "loss: 0.595334 [51264/60000]\n",
      "loss: 0.428986 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.8%, Avg loss: 0.484549\n",
      "\n",
      "Epoch 53\n",
      "---------------------------------\n",
      "loss: 0.338692 [   64/60000]\n",
      "loss: 0.490268 [ 6464/60000]\n",
      "loss: 0.311292 [12864/60000]\n",
      "loss: 0.530138 [19264/60000]\n",
      "loss: 0.460389 [25664/60000]\n",
      "loss: 0.439048 [32064/60000]\n",
      "loss: 0.469044 [38464/60000]\n",
      "loss: 0.648420 [44864/60000]\n",
      "loss: 0.593777 [51264/60000]\n",
      "loss: 0.426564 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 82.9%, Avg loss: 0.482941\n",
      "\n",
      "Epoch 54\n",
      "---------------------------------\n",
      "loss: 0.335472 [   64/60000]\n",
      "loss: 0.488350 [ 6464/60000]\n",
      "loss: 0.309431 [12864/60000]\n",
      "loss: 0.527683 [19264/60000]\n",
      "loss: 0.457836 [25664/60000]\n",
      "loss: 0.436717 [32064/60000]\n",
      "loss: 0.466927 [38464/60000]\n",
      "loss: 0.646926 [44864/60000]\n",
      "loss: 0.592229 [51264/60000]\n",
      "loss: 0.424254 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.0%, Avg loss: 0.481381\n",
      "\n",
      "Epoch 55\n",
      "---------------------------------\n",
      "loss: 0.332360 [   64/60000]\n",
      "loss: 0.486515 [ 6464/60000]\n",
      "loss: 0.307625 [12864/60000]\n",
      "loss: 0.525342 [19264/60000]\n",
      "loss: 0.455372 [25664/60000]\n",
      "loss: 0.434460 [32064/60000]\n",
      "loss: 0.464871 [38464/60000]\n",
      "loss: 0.645429 [44864/60000]\n",
      "loss: 0.590644 [51264/60000]\n",
      "loss: 0.422054 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.0%, Avg loss: 0.479867\n",
      "\n",
      "Epoch 56\n",
      "---------------------------------\n",
      "loss: 0.329335 [   64/60000]\n",
      "loss: 0.484732 [ 6464/60000]\n",
      "loss: 0.305878 [12864/60000]\n",
      "loss: 0.523081 [19264/60000]\n",
      "loss: 0.452935 [25664/60000]\n",
      "loss: 0.432329 [32064/60000]\n",
      "loss: 0.462889 [38464/60000]\n",
      "loss: 0.643890 [44864/60000]\n",
      "loss: 0.589073 [51264/60000]\n",
      "loss: 0.419985 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.0%, Avg loss: 0.478396\n",
      "\n",
      "Epoch 57\n",
      "---------------------------------\n",
      "loss: 0.326393 [   64/60000]\n",
      "loss: 0.482972 [ 6464/60000]\n",
      "loss: 0.304142 [12864/60000]\n",
      "loss: 0.520927 [19264/60000]\n",
      "loss: 0.450564 [25664/60000]\n",
      "loss: 0.430285 [32064/60000]\n",
      "loss: 0.460933 [38464/60000]\n",
      "loss: 0.642292 [44864/60000]\n",
      "loss: 0.587522 [51264/60000]\n",
      "loss: 0.418056 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.0%, Avg loss: 0.476961\n",
      "\n",
      "Epoch 58\n",
      "---------------------------------\n",
      "loss: 0.323517 [   64/60000]\n",
      "loss: 0.481270 [ 6464/60000]\n",
      "loss: 0.302445 [12864/60000]\n",
      "loss: 0.518827 [19264/60000]\n",
      "loss: 0.448277 [25664/60000]\n",
      "loss: 0.428317 [32064/60000]\n",
      "loss: 0.458996 [38464/60000]\n",
      "loss: 0.640684 [44864/60000]\n",
      "loss: 0.585999 [51264/60000]\n",
      "loss: 0.416228 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.1%, Avg loss: 0.475564\n",
      "\n",
      "Epoch 59\n",
      "---------------------------------\n",
      "loss: 0.320683 [   64/60000]\n",
      "loss: 0.479593 [ 6464/60000]\n",
      "loss: 0.300799 [12864/60000]\n",
      "loss: 0.516809 [19264/60000]\n",
      "loss: 0.446040 [25664/60000]\n",
      "loss: 0.426419 [32064/60000]\n",
      "loss: 0.457148 [38464/60000]\n",
      "loss: 0.639013 [44864/60000]\n",
      "loss: 0.584453 [51264/60000]\n",
      "loss: 0.414420 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.2%, Avg loss: 0.474198\n",
      "\n",
      "Epoch 60\n",
      "---------------------------------\n",
      "loss: 0.317983 [   64/60000]\n",
      "loss: 0.477912 [ 6464/60000]\n",
      "loss: 0.299200 [12864/60000]\n",
      "loss: 0.514857 [19264/60000]\n",
      "loss: 0.443913 [25664/60000]\n",
      "loss: 0.424602 [32064/60000]\n",
      "loss: 0.455331 [38464/60000]\n",
      "loss: 0.637341 [44864/60000]\n",
      "loss: 0.582879 [51264/60000]\n",
      "loss: 0.412680 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.2%, Avg loss: 0.472866\n",
      "\n",
      "Epoch 61\n",
      "---------------------------------\n",
      "loss: 0.315373 [   64/60000]\n",
      "loss: 0.476301 [ 6464/60000]\n",
      "loss: 0.297612 [12864/60000]\n",
      "loss: 0.512972 [19264/60000]\n",
      "loss: 0.441793 [25664/60000]\n",
      "loss: 0.422873 [32064/60000]\n",
      "loss: 0.453611 [38464/60000]\n",
      "loss: 0.635724 [44864/60000]\n",
      "loss: 0.581342 [51264/60000]\n",
      "loss: 0.410996 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.2%, Avg loss: 0.471564\n",
      "\n",
      "Epoch 62\n",
      "---------------------------------\n",
      "loss: 0.312853 [   64/60000]\n",
      "loss: 0.474688 [ 6464/60000]\n",
      "loss: 0.296059 [12864/60000]\n",
      "loss: 0.511075 [19264/60000]\n",
      "loss: 0.439688 [25664/60000]\n",
      "loss: 0.421136 [32064/60000]\n",
      "loss: 0.451953 [38464/60000]\n",
      "loss: 0.634018 [44864/60000]\n",
      "loss: 0.579793 [51264/60000]\n",
      "loss: 0.409343 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.2%, Avg loss: 0.470295\n",
      "\n",
      "Epoch 63\n",
      "---------------------------------\n",
      "loss: 0.310401 [   64/60000]\n",
      "loss: 0.473102 [ 6464/60000]\n",
      "loss: 0.294567 [12864/60000]\n",
      "loss: 0.509211 [19264/60000]\n",
      "loss: 0.437533 [25664/60000]\n",
      "loss: 0.419496 [32064/60000]\n",
      "loss: 0.450326 [38464/60000]\n",
      "loss: 0.632258 [44864/60000]\n",
      "loss: 0.578239 [51264/60000]\n",
      "loss: 0.407797 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.3%, Avg loss: 0.469059\n",
      "\n",
      "Epoch 64\n",
      "---------------------------------\n",
      "loss: 0.308013 [   64/60000]\n",
      "loss: 0.471547 [ 6464/60000]\n",
      "loss: 0.293137 [12864/60000]\n",
      "loss: 0.507436 [19264/60000]\n",
      "loss: 0.435413 [25664/60000]\n",
      "loss: 0.417931 [32064/60000]\n",
      "loss: 0.448724 [38464/60000]\n",
      "loss: 0.630569 [44864/60000]\n",
      "loss: 0.576750 [51264/60000]\n",
      "loss: 0.406350 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.4%, Avg loss: 0.467853\n",
      "\n",
      "Epoch 65\n",
      "---------------------------------\n",
      "loss: 0.305696 [   64/60000]\n",
      "loss: 0.469992 [ 6464/60000]\n",
      "loss: 0.291719 [12864/60000]\n",
      "loss: 0.505718 [19264/60000]\n",
      "loss: 0.433326 [25664/60000]\n",
      "loss: 0.416423 [32064/60000]\n",
      "loss: 0.447142 [38464/60000]\n",
      "loss: 0.628891 [44864/60000]\n",
      "loss: 0.575277 [51264/60000]\n",
      "loss: 0.404958 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.5%, Avg loss: 0.466671\n",
      "\n",
      "Epoch 66\n",
      "---------------------------------\n",
      "loss: 0.303442 [   64/60000]\n",
      "loss: 0.468462 [ 6464/60000]\n",
      "loss: 0.290356 [12864/60000]\n",
      "loss: 0.504038 [19264/60000]\n",
      "loss: 0.431279 [25664/60000]\n",
      "loss: 0.415017 [32064/60000]\n",
      "loss: 0.445582 [38464/60000]\n",
      "loss: 0.627224 [44864/60000]\n",
      "loss: 0.573828 [51264/60000]\n",
      "loss: 0.403653 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.5%, Avg loss: 0.465511\n",
      "\n",
      "Epoch 67\n",
      "---------------------------------\n",
      "loss: 0.301254 [   64/60000]\n",
      "loss: 0.466970 [ 6464/60000]\n",
      "loss: 0.288996 [12864/60000]\n",
      "loss: 0.502392 [19264/60000]\n",
      "loss: 0.429217 [25664/60000]\n",
      "loss: 0.413610 [32064/60000]\n",
      "loss: 0.444058 [38464/60000]\n",
      "loss: 0.625623 [44864/60000]\n",
      "loss: 0.572374 [51264/60000]\n",
      "loss: 0.402393 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.5%, Avg loss: 0.464376\n",
      "\n",
      "Epoch 68\n",
      "---------------------------------\n",
      "loss: 0.299094 [   64/60000]\n",
      "loss: 0.465470 [ 6464/60000]\n",
      "loss: 0.287679 [12864/60000]\n",
      "loss: 0.500779 [19264/60000]\n",
      "loss: 0.427229 [25664/60000]\n",
      "loss: 0.412239 [32064/60000]\n",
      "loss: 0.442577 [38464/60000]\n",
      "loss: 0.624022 [44864/60000]\n",
      "loss: 0.570987 [51264/60000]\n",
      "loss: 0.401183 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.5%, Avg loss: 0.463268\n",
      "\n",
      "Epoch 69\n",
      "---------------------------------\n",
      "loss: 0.297003 [   64/60000]\n",
      "loss: 0.464000 [ 6464/60000]\n",
      "loss: 0.286383 [12864/60000]\n",
      "loss: 0.499241 [19264/60000]\n",
      "loss: 0.425281 [25664/60000]\n",
      "loss: 0.410949 [32064/60000]\n",
      "loss: 0.441154 [38464/60000]\n",
      "loss: 0.622409 [44864/60000]\n",
      "loss: 0.569591 [51264/60000]\n",
      "loss: 0.399993 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.6%, Avg loss: 0.462177\n",
      "\n",
      "Epoch 70\n",
      "---------------------------------\n",
      "loss: 0.294977 [   64/60000]\n",
      "loss: 0.462524 [ 6464/60000]\n",
      "loss: 0.285110 [12864/60000]\n",
      "loss: 0.497713 [19264/60000]\n",
      "loss: 0.423354 [25664/60000]\n",
      "loss: 0.409681 [32064/60000]\n",
      "loss: 0.439735 [38464/60000]\n",
      "loss: 0.620771 [44864/60000]\n",
      "loss: 0.568228 [51264/60000]\n",
      "loss: 0.398879 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.6%, Avg loss: 0.461106\n",
      "\n",
      "Epoch 71\n",
      "---------------------------------\n",
      "loss: 0.292993 [   64/60000]\n",
      "loss: 0.461082 [ 6464/60000]\n",
      "loss: 0.283843 [12864/60000]\n",
      "loss: 0.496188 [19264/60000]\n",
      "loss: 0.421451 [25664/60000]\n",
      "loss: 0.408434 [32064/60000]\n",
      "loss: 0.438364 [38464/60000]\n",
      "loss: 0.619116 [44864/60000]\n",
      "loss: 0.566898 [51264/60000]\n",
      "loss: 0.397835 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.6%, Avg loss: 0.460054\n",
      "\n",
      "Epoch 72\n",
      "---------------------------------\n",
      "loss: 0.291088 [   64/60000]\n",
      "loss: 0.459680 [ 6464/60000]\n",
      "loss: 0.282616 [12864/60000]\n",
      "loss: 0.494711 [19264/60000]\n",
      "loss: 0.419609 [25664/60000]\n",
      "loss: 0.407198 [32064/60000]\n",
      "loss: 0.437032 [38464/60000]\n",
      "loss: 0.617491 [44864/60000]\n",
      "loss: 0.565579 [51264/60000]\n",
      "loss: 0.396814 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.7%, Avg loss: 0.459028\n",
      "\n",
      "Epoch 73\n",
      "---------------------------------\n",
      "loss: 0.289232 [   64/60000]\n",
      "loss: 0.458274 [ 6464/60000]\n",
      "loss: 0.281429 [12864/60000]\n",
      "loss: 0.493258 [19264/60000]\n",
      "loss: 0.417754 [25664/60000]\n",
      "loss: 0.405955 [32064/60000]\n",
      "loss: 0.435723 [38464/60000]\n",
      "loss: 0.615900 [44864/60000]\n",
      "loss: 0.564255 [51264/60000]\n",
      "loss: 0.395844 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.7%, Avg loss: 0.458017\n",
      "\n",
      "Epoch 74\n",
      "---------------------------------\n",
      "loss: 0.287443 [   64/60000]\n",
      "loss: 0.456899 [ 6464/60000]\n",
      "loss: 0.280253 [12864/60000]\n",
      "loss: 0.491797 [19264/60000]\n",
      "loss: 0.415853 [25664/60000]\n",
      "loss: 0.404818 [32064/60000]\n",
      "loss: 0.434431 [38464/60000]\n",
      "loss: 0.614341 [44864/60000]\n",
      "loss: 0.562937 [51264/60000]\n",
      "loss: 0.394907 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.8%, Avg loss: 0.457018\n",
      "\n",
      "Epoch 75\n",
      "---------------------------------\n",
      "loss: 0.285695 [   64/60000]\n",
      "loss: 0.455526 [ 6464/60000]\n",
      "loss: 0.279116 [12864/60000]\n",
      "loss: 0.490319 [19264/60000]\n",
      "loss: 0.414039 [25664/60000]\n",
      "loss: 0.403669 [32064/60000]\n",
      "loss: 0.433143 [38464/60000]\n",
      "loss: 0.612794 [44864/60000]\n",
      "loss: 0.561653 [51264/60000]\n",
      "loss: 0.394015 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.8%, Avg loss: 0.456036\n",
      "\n",
      "Epoch 76\n",
      "---------------------------------\n",
      "loss: 0.284050 [   64/60000]\n",
      "loss: 0.454190 [ 6464/60000]\n",
      "loss: 0.277998 [12864/60000]\n",
      "loss: 0.488914 [19264/60000]\n",
      "loss: 0.412300 [25664/60000]\n",
      "loss: 0.402537 [32064/60000]\n",
      "loss: 0.431946 [38464/60000]\n",
      "loss: 0.611229 [44864/60000]\n",
      "loss: 0.560388 [51264/60000]\n",
      "loss: 0.393158 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.8%, Avg loss: 0.455070\n",
      "\n",
      "Epoch 77\n",
      "---------------------------------\n",
      "loss: 0.282436 [   64/60000]\n",
      "loss: 0.452840 [ 6464/60000]\n",
      "loss: 0.276905 [12864/60000]\n",
      "loss: 0.487514 [19264/60000]\n",
      "loss: 0.410580 [25664/60000]\n",
      "loss: 0.401401 [32064/60000]\n",
      "loss: 0.430763 [38464/60000]\n",
      "loss: 0.609620 [44864/60000]\n",
      "loss: 0.559139 [51264/60000]\n",
      "loss: 0.392319 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.9%, Avg loss: 0.454120\n",
      "\n",
      "Epoch 78\n",
      "---------------------------------\n",
      "loss: 0.280878 [   64/60000]\n",
      "loss: 0.451552 [ 6464/60000]\n",
      "loss: 0.275818 [12864/60000]\n",
      "loss: 0.486139 [19264/60000]\n",
      "loss: 0.408864 [25664/60000]\n",
      "loss: 0.400204 [32064/60000]\n",
      "loss: 0.429637 [38464/60000]\n",
      "loss: 0.608103 [44864/60000]\n",
      "loss: 0.557920 [51264/60000]\n",
      "loss: 0.391472 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.9%, Avg loss: 0.453185\n",
      "\n",
      "Epoch 79\n",
      "---------------------------------\n",
      "loss: 0.279336 [   64/60000]\n",
      "loss: 0.450262 [ 6464/60000]\n",
      "loss: 0.274760 [12864/60000]\n",
      "loss: 0.484707 [19264/60000]\n",
      "loss: 0.407102 [25664/60000]\n",
      "loss: 0.399028 [32064/60000]\n",
      "loss: 0.428529 [38464/60000]\n",
      "loss: 0.606576 [44864/60000]\n",
      "loss: 0.556668 [51264/60000]\n",
      "loss: 0.390642 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 83.9%, Avg loss: 0.452262\n",
      "\n",
      "Epoch 80\n",
      "---------------------------------\n",
      "loss: 0.277862 [   64/60000]\n",
      "loss: 0.448999 [ 6464/60000]\n",
      "loss: 0.273729 [12864/60000]\n",
      "loss: 0.483334 [19264/60000]\n",
      "loss: 0.405367 [25664/60000]\n",
      "loss: 0.397897 [32064/60000]\n",
      "loss: 0.427480 [38464/60000]\n",
      "loss: 0.605114 [44864/60000]\n",
      "loss: 0.555406 [51264/60000]\n",
      "loss: 0.389837 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.0%, Avg loss: 0.451355\n",
      "\n",
      "Epoch 81\n",
      "---------------------------------\n",
      "loss: 0.276428 [   64/60000]\n",
      "loss: 0.447704 [ 6464/60000]\n",
      "loss: 0.272726 [12864/60000]\n",
      "loss: 0.481941 [19264/60000]\n",
      "loss: 0.403603 [25664/60000]\n",
      "loss: 0.396838 [32064/60000]\n",
      "loss: 0.426434 [38464/60000]\n",
      "loss: 0.603666 [44864/60000]\n",
      "loss: 0.554180 [51264/60000]\n",
      "loss: 0.389107 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.0%, Avg loss: 0.450460\n",
      "\n",
      "Epoch 82\n",
      "---------------------------------\n",
      "loss: 0.275061 [   64/60000]\n",
      "loss: 0.446461 [ 6464/60000]\n",
      "loss: 0.271745 [12864/60000]\n",
      "loss: 0.480574 [19264/60000]\n",
      "loss: 0.401912 [25664/60000]\n",
      "loss: 0.395794 [32064/60000]\n",
      "loss: 0.425363 [38464/60000]\n",
      "loss: 0.602270 [44864/60000]\n",
      "loss: 0.552976 [51264/60000]\n",
      "loss: 0.388386 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.1%, Avg loss: 0.449575\n",
      "\n",
      "Epoch 83\n",
      "---------------------------------\n",
      "loss: 0.273742 [   64/60000]\n",
      "loss: 0.445259 [ 6464/60000]\n",
      "loss: 0.270806 [12864/60000]\n",
      "loss: 0.479172 [19264/60000]\n",
      "loss: 0.400271 [25664/60000]\n",
      "loss: 0.394762 [32064/60000]\n",
      "loss: 0.424267 [38464/60000]\n",
      "loss: 0.600934 [44864/60000]\n",
      "loss: 0.551767 [51264/60000]\n",
      "loss: 0.387662 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.1%, Avg loss: 0.448703\n",
      "\n",
      "Epoch 84\n",
      "---------------------------------\n",
      "loss: 0.272478 [   64/60000]\n",
      "loss: 0.444057 [ 6464/60000]\n",
      "loss: 0.269862 [12864/60000]\n",
      "loss: 0.477783 [19264/60000]\n",
      "loss: 0.398647 [25664/60000]\n",
      "loss: 0.393746 [32064/60000]\n",
      "loss: 0.423202 [38464/60000]\n",
      "loss: 0.599588 [44864/60000]\n",
      "loss: 0.550592 [51264/60000]\n",
      "loss: 0.386945 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.2%, Avg loss: 0.447843\n",
      "\n",
      "Epoch 85\n",
      "---------------------------------\n",
      "loss: 0.271233 [   64/60000]\n",
      "loss: 0.442862 [ 6464/60000]\n",
      "loss: 0.268911 [12864/60000]\n",
      "loss: 0.476422 [19264/60000]\n",
      "loss: 0.397055 [25664/60000]\n",
      "loss: 0.392797 [32064/60000]\n",
      "loss: 0.422133 [38464/60000]\n",
      "loss: 0.598284 [44864/60000]\n",
      "loss: 0.549396 [51264/60000]\n",
      "loss: 0.386263 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.2%, Avg loss: 0.446992\n",
      "\n",
      "Epoch 86\n",
      "---------------------------------\n",
      "loss: 0.270026 [   64/60000]\n",
      "loss: 0.441682 [ 6464/60000]\n",
      "loss: 0.268002 [12864/60000]\n",
      "loss: 0.475120 [19264/60000]\n",
      "loss: 0.395463 [25664/60000]\n",
      "loss: 0.391801 [32064/60000]\n",
      "loss: 0.421046 [38464/60000]\n",
      "loss: 0.597014 [44864/60000]\n",
      "loss: 0.548148 [51264/60000]\n",
      "loss: 0.385572 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.2%, Avg loss: 0.446151\n",
      "\n",
      "Epoch 87\n",
      "---------------------------------\n",
      "loss: 0.268870 [   64/60000]\n",
      "loss: 0.440529 [ 6464/60000]\n",
      "loss: 0.267130 [12864/60000]\n",
      "loss: 0.473805 [19264/60000]\n",
      "loss: 0.393963 [25664/60000]\n",
      "loss: 0.390795 [32064/60000]\n",
      "loss: 0.419953 [38464/60000]\n",
      "loss: 0.595692 [44864/60000]\n",
      "loss: 0.546862 [51264/60000]\n",
      "loss: 0.384960 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.2%, Avg loss: 0.445318\n",
      "\n",
      "Epoch 88\n",
      "---------------------------------\n",
      "loss: 0.267761 [   64/60000]\n",
      "loss: 0.439370 [ 6464/60000]\n",
      "loss: 0.266286 [12864/60000]\n",
      "loss: 0.472484 [19264/60000]\n",
      "loss: 0.392390 [25664/60000]\n",
      "loss: 0.389865 [32064/60000]\n",
      "loss: 0.418825 [38464/60000]\n",
      "loss: 0.594360 [44864/60000]\n",
      "loss: 0.545553 [51264/60000]\n",
      "loss: 0.384344 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.2%, Avg loss: 0.444495\n",
      "\n",
      "Epoch 89\n",
      "---------------------------------\n",
      "loss: 0.266687 [   64/60000]\n",
      "loss: 0.438210 [ 6464/60000]\n",
      "loss: 0.265397 [12864/60000]\n",
      "loss: 0.471187 [19264/60000]\n",
      "loss: 0.390830 [25664/60000]\n",
      "loss: 0.388955 [32064/60000]\n",
      "loss: 0.417730 [38464/60000]\n",
      "loss: 0.593034 [44864/60000]\n",
      "loss: 0.544273 [51264/60000]\n",
      "loss: 0.383721 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.2%, Avg loss: 0.443682\n",
      "\n",
      "Epoch 90\n",
      "---------------------------------\n",
      "loss: 0.265665 [   64/60000]\n",
      "loss: 0.437014 [ 6464/60000]\n",
      "loss: 0.264531 [12864/60000]\n",
      "loss: 0.469950 [19264/60000]\n",
      "loss: 0.389268 [25664/60000]\n",
      "loss: 0.388047 [32064/60000]\n",
      "loss: 0.416771 [38464/60000]\n",
      "loss: 0.591746 [44864/60000]\n",
      "loss: 0.542984 [51264/60000]\n",
      "loss: 0.383113 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.3%, Avg loss: 0.442887\n",
      "\n",
      "Epoch 91\n",
      "---------------------------------\n",
      "loss: 0.264703 [   64/60000]\n",
      "loss: 0.435847 [ 6464/60000]\n",
      "loss: 0.263727 [12864/60000]\n",
      "loss: 0.468670 [19264/60000]\n",
      "loss: 0.387715 [25664/60000]\n",
      "loss: 0.387131 [32064/60000]\n",
      "loss: 0.415770 [38464/60000]\n",
      "loss: 0.590501 [44864/60000]\n",
      "loss: 0.541784 [51264/60000]\n",
      "loss: 0.382565 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.3%, Avg loss: 0.442102\n",
      "\n",
      "Epoch 92\n",
      "---------------------------------\n",
      "loss: 0.263758 [   64/60000]\n",
      "loss: 0.434708 [ 6464/60000]\n",
      "loss: 0.262927 [12864/60000]\n",
      "loss: 0.467444 [19264/60000]\n",
      "loss: 0.386188 [25664/60000]\n",
      "loss: 0.386236 [32064/60000]\n",
      "loss: 0.414728 [38464/60000]\n",
      "loss: 0.589194 [44864/60000]\n",
      "loss: 0.540660 [51264/60000]\n",
      "loss: 0.382019 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.3%, Avg loss: 0.441324\n",
      "\n",
      "Epoch 93\n",
      "---------------------------------\n",
      "loss: 0.262778 [   64/60000]\n",
      "loss: 0.433571 [ 6464/60000]\n",
      "loss: 0.262148 [12864/60000]\n",
      "loss: 0.466146 [19264/60000]\n",
      "loss: 0.384716 [25664/60000]\n",
      "loss: 0.385355 [32064/60000]\n",
      "loss: 0.413684 [38464/60000]\n",
      "loss: 0.587879 [44864/60000]\n",
      "loss: 0.539596 [51264/60000]\n",
      "loss: 0.381505 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.3%, Avg loss: 0.440556\n",
      "\n",
      "Epoch 94\n",
      "---------------------------------\n",
      "loss: 0.261829 [   64/60000]\n",
      "loss: 0.432470 [ 6464/60000]\n",
      "loss: 0.261388 [12864/60000]\n",
      "loss: 0.464922 [19264/60000]\n",
      "loss: 0.383262 [25664/60000]\n",
      "loss: 0.384449 [32064/60000]\n",
      "loss: 0.412672 [38464/60000]\n",
      "loss: 0.586595 [44864/60000]\n",
      "loss: 0.538537 [51264/60000]\n",
      "loss: 0.380956 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.3%, Avg loss: 0.439795\n",
      "\n",
      "Epoch 95\n",
      "---------------------------------\n",
      "loss: 0.260936 [   64/60000]\n",
      "loss: 0.431392 [ 6464/60000]\n",
      "loss: 0.260632 [12864/60000]\n",
      "loss: 0.463698 [19264/60000]\n",
      "loss: 0.381808 [25664/60000]\n",
      "loss: 0.383582 [32064/60000]\n",
      "loss: 0.411668 [38464/60000]\n",
      "loss: 0.585357 [44864/60000]\n",
      "loss: 0.537506 [51264/60000]\n",
      "loss: 0.380473 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.3%, Avg loss: 0.439048\n",
      "\n",
      "Epoch 96\n",
      "---------------------------------\n",
      "loss: 0.260045 [   64/60000]\n",
      "loss: 0.430276 [ 6464/60000]\n",
      "loss: 0.259860 [12864/60000]\n",
      "loss: 0.462509 [19264/60000]\n",
      "loss: 0.380362 [25664/60000]\n",
      "loss: 0.382703 [32064/60000]\n",
      "loss: 0.410653 [38464/60000]\n",
      "loss: 0.584104 [44864/60000]\n",
      "loss: 0.536443 [51264/60000]\n",
      "loss: 0.379975 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.3%, Avg loss: 0.438306\n",
      "\n",
      "Epoch 97\n",
      "---------------------------------\n",
      "loss: 0.259188 [   64/60000]\n",
      "loss: 0.429211 [ 6464/60000]\n",
      "loss: 0.259142 [12864/60000]\n",
      "loss: 0.461334 [19264/60000]\n",
      "loss: 0.378977 [25664/60000]\n",
      "loss: 0.381788 [32064/60000]\n",
      "loss: 0.409654 [38464/60000]\n",
      "loss: 0.582817 [44864/60000]\n",
      "loss: 0.535382 [51264/60000]\n",
      "loss: 0.379502 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.4%, Avg loss: 0.437580\n",
      "\n",
      "Epoch 98\n",
      "---------------------------------\n",
      "loss: 0.258349 [   64/60000]\n",
      "loss: 0.428117 [ 6464/60000]\n",
      "loss: 0.258380 [12864/60000]\n",
      "loss: 0.460153 [19264/60000]\n",
      "loss: 0.377621 [25664/60000]\n",
      "loss: 0.380947 [32064/60000]\n",
      "loss: 0.408668 [38464/60000]\n",
      "loss: 0.581566 [44864/60000]\n",
      "loss: 0.534370 [51264/60000]\n",
      "loss: 0.379050 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.4%, Avg loss: 0.436854\n",
      "\n",
      "Epoch 99\n",
      "---------------------------------\n",
      "loss: 0.257538 [   64/60000]\n",
      "loss: 0.427023 [ 6464/60000]\n",
      "loss: 0.257646 [12864/60000]\n",
      "loss: 0.459002 [19264/60000]\n",
      "loss: 0.376312 [25664/60000]\n",
      "loss: 0.380041 [32064/60000]\n",
      "loss: 0.407740 [38464/60000]\n",
      "loss: 0.580256 [44864/60000]\n",
      "loss: 0.533412 [51264/60000]\n",
      "loss: 0.378599 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.5%, Avg loss: 0.436139\n",
      "\n",
      "Epoch 100\n",
      "---------------------------------\n",
      "loss: 0.256759 [   64/60000]\n",
      "loss: 0.425974 [ 6464/60000]\n",
      "loss: 0.256933 [12864/60000]\n",
      "loss: 0.457825 [19264/60000]\n",
      "loss: 0.374952 [25664/60000]\n",
      "loss: 0.379118 [32064/60000]\n",
      "loss: 0.406773 [38464/60000]\n",
      "loss: 0.579003 [44864/60000]\n",
      "loss: 0.532477 [51264/60000]\n",
      "loss: 0.378125 [57664/60000]\n",
      "Test Error: \n",
      " ACC: 84.5%, Avg loss: 0.435425\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
